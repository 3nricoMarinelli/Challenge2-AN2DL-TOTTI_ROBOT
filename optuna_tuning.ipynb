{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000,), (48000, 2776), (48000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Array che contiene le categorie di ogni time series. Dim 48000x1\n",
    "categories = np.load(\"categories.npy\")\n",
    "#Array che contiene i valori delle time series. Dim 48000x2776\n",
    "training_data = np.load(\"training_data.npy\")\n",
    "#Array che contiene gli indici che descrivono gli estremi degli intervalli validi delle time series. Dim 48000x2\n",
    "valid_periods = np.load(\"valid_periods.npy\")\n",
    "\n",
    "categories.shape, training_data.shape, valid_periods.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(training_data, garbage_threshold = 600):\n",
    "    cleaned_data = []\n",
    "    cleaned_categories = []\n",
    "    cleaned_valid_periods = []\n",
    "    for i in range(training_data.shape[0]):\n",
    "        if valid_periods[i][1] - valid_periods[i][0] >= garbage_threshold:\n",
    "            cleaned_data.append(training_data[i])\n",
    "            cleaned_categories.append(categories[i])\n",
    "            cleaned_valid_periods.append(valid_periods[i])\n",
    "    return np.array(cleaned_data), np.array(cleaned_categories), np.array(cleaned_valid_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_for_categories(training_data, categories):\n",
    "    all_samples_list = []\n",
    "    for i in np.unique(categories):\n",
    "        mask = np.where(categories == i, True, False)\n",
    "\n",
    "        samples_for_category = training_data[mask]\n",
    "        all_samples_list.append(samples_for_category)\n",
    "    return np.array(all_samples_list[0]), np.array(all_samples_list[1]), np.array(all_samples_list[2]), np.array(all_samples_list[3]), np.array(all_samples_list[4]), np.array(all_samples_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((609,), (609, 2776), (609, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data, cleaned_categories, cleaned_valid_periods = clean_data(training_data)\n",
    "\n",
    "cleaned_categories.shape, cleaned_data.shape, cleaned_valid_periods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82, 2776), (106, 2776), (190, 2776), (160, 2776), (64, 2776), (7, 2776))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_A, data_B, data_C, data_D, data_E, data_F = split_for_categories(cleaned_data, cleaned_categories)\n",
    "\n",
    "data_A.shape, data_B.shape, data_C.shape, data_D.shape, data_E.shape, data_F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_ratio = 0.8):\n",
    "    data = data\n",
    "    \n",
    "    rows_to_select = int(len(data) * train_ratio)\n",
    "    training_set = data[:rows_to_select, :]\n",
    "    test_set = data[rows_to_select:, :]\n",
    "    \n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 26, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation_D, test_D = train_test_split(data_D)\n",
    "train_D, validation_D = train_test_split(train_validation_D)\n",
    "len(train_D), len(validation_D), len(test_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, window=200, stride=1, telescope=9):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.copy()\n",
    "    padding_check = df.size%window\n",
    "\n",
    "    #print(temp_df.size)\n",
    "\n",
    "    if(padding_check != 0):\n",
    "        # Compute padding length\n",
    "        padding_len = window - df.size%window\n",
    "        padding = np.zeros((padding_len), dtype='float32')\n",
    "        temp_df = np.concatenate((padding,df))\n",
    "        assert temp_df.size % window == 0\n",
    "\n",
    "    #print(temp_df.size)\n",
    "    for idx in np.arange(0,temp_df.size-window-telescope,stride):\n",
    "        dataset.append(temp_df[idx:idx+window])\n",
    "        labels.append(temp_df[idx+window:idx+window+telescope])\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is a pandas series containing 48000 lists (either training, validation or test)\n",
    "def THE_SEQUENCER(data):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        time_series = data[i][valid_periods[i][0]:valid_periods[i][1]]\n",
    "        dset, labs = build_sequences(time_series)\n",
    "        if len(dset) == 0:\n",
    "            continue\n",
    "        dataset.append(dset)\n",
    "        labels.append(labs)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_D, train_labels_D = THE_SEQUENCER(train_D)\n",
    "validation_sequences_D, validation_labels_D = THE_SEQUENCER(train_D)\n",
    "test_sequences_D, test_labels_D = THE_SEQUENCER(data_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CONV_LSTM_model(input_shape, output_shape, num_neurons):\n",
    "    # Ensure the input time steps are at least as many as the output time steps\n",
    "    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n",
    "    # Define the input layer with the specified shape\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "    # Add a Bidirectional LSTM layer with 64 units\n",
    "\n",
    "    x = tfkl.Bidirectional(tfkl.LSTM(num_neurons, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
    "    \n",
    "    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n",
    "    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
    "    # Add a final Convolution layer to match the desired output shape\n",
    "    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n",
    "    # Calculate the size to crop from the output to match the output shape\n",
    "    crop_size = output_layer.shape[1] - output_shape[0]\n",
    "    # Crop the output to the desired length\n",
    "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
    "    # Construct the model by connecting input and output layers\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
    "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(train_sequences_D)):\n",
    "    for j in range(len(train_sequences_D[i])):\n",
    "        X_train.append(train_sequences_D[i][j])\n",
    "        y_train.append(train_labels_D[i][j])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = []\n",
    "y_valid = []\n",
    "for i in range(len(validation_sequences_D)):\n",
    "    for j in range(len(validation_sequences_D[i])):\n",
    "        X_valid.append(validation_sequences_D[i][j])\n",
    "        y_valid.append(validation_labels_D[i][j])\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4c/6a/219a431aaf81b3eb3070fd2d58116baa366d3072f43bbcc87dc3495b7546/optuna-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/d5/d8/fc331ad9aa5f2a551042582c3ededd70ee4e72b032089b1784150a5704ac/alembic-1.13.0-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/95/df/520663eb7f7a329f7c585834b754bcc3cbcc03957d85fcbba4a2a723ad9d/colorlog-6.8.0-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.8.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pippolazza/Library/Python/3.11/lib/python/site-packages (from optuna) (23.1)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna)\n",
      "  Obtaining dependency information for sqlalchemy>=1.3.0 from https://files.pythonhosted.org/packages/c7/55/d1d2ad054fb7e9188681d56df40ed81c2c198314a805b180b0ec99019da1/SQLAlchemy-2.0.23-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading SQLAlchemy-2.0.23-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/24/3b/11fe92d68c6a42468ddab0cf03f454419b0788fff4e91ba46b8bebafeffd/Mako-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached Mako-1.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.23-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Using cached Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0 sqlalchemy-2.0.23\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "input_shape = (200,1)\n",
    "output_shape = (9,1)\n",
    "\n",
    "def objective_function(optuna_trial):\n",
    "\n",
    "    # Generate our trial model.\n",
    "\n",
    "    num_neurons = optuna_trial.suggest_int(\"num_neurons\", 10, 128)\n",
    "    batch_size = optuna_trial.suggest_int(\"batch_size\", 32, 128)\n",
    "\n",
    "    print(f\"Current trial parameters: num_neurons={num_neurons}, batch_size={batch_size}\")\n",
    "\n",
    "    model = build_CONV_LSTM_model(input_shape, output_shape, num_neurons)\n",
    "\n",
    "    # Fit the model on the training data.\n",
    "    # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
    "    model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size, \n",
    "    epochs = 200,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveResults(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_df = pd.DataFrame()\n",
    "    \n",
    "    def __call__(self, optuna_study, optuna_trial):\n",
    "        hyperparam_dict = optuna_trial.params.copy()\n",
    "        hyperparam_dict[\"result\"] = optuna_trial.values[0]\n",
    "        \n",
    "        self.results_df = self.results_df.append(hyperparam_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-14 22:38:49,290] A new study created in memory with name: no-name-ad97d9e0-e60b-4ce4-b0d5-09633f912df2\n"
     ]
    }
   ],
   "source": [
    "optuna_study = optuna.create_study(direction=\"minimize\")\n",
    "save_results = SaveResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_study.optimize(objective_function,\n",
    "                    callbacks=[save_results],\n",
    "                    n_trials = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
