{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array che contiene le categorie di ogni time series. Dim 48000x1\n",
    "categories = np.load(\"categories.npy\")\n",
    "#Array che contiene i valori delle time series. Dim 48000x2776\n",
    "training_data = np.load(\"training_data.npy\")\n",
    "#Array che contiene gli indici che descrivono gli estremi degli intervalli validi delle time series. Dim 48000x2\n",
    "valid_periods = np.load(\"valid_periods.npy\")\n",
    "\n",
    "categories.shape, training_data.shape, valid_periods.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(training_data, garbage_threshold = 600):\n",
    "    cleaned_data = []\n",
    "    cleaned_categories = []\n",
    "    cleaned_valid_periods = []\n",
    "    for i in range(training_data.shape[0]):\n",
    "        if valid_periods[i][1] - valid_periods[i][0] >= garbage_threshold:\n",
    "            cleaned_data.append(training_data[i])\n",
    "            cleaned_categories.append(categories[i])\n",
    "            cleaned_valid_periods.append(valid_periods[i])\n",
    "    return np.array(cleaned_data), np.array(cleaned_categories), np.array(cleaned_valid_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_for_categories(training_data, categories):\n",
    "    all_samples_list = []\n",
    "    for i in np.unique(categories):\n",
    "        mask = np.where(categories == i, True, False)\n",
    "\n",
    "        samples_for_category = training_data[mask]\n",
    "        all_samples_list.append(samples_for_category)\n",
    "    return np.array(all_samples_list[0]), np.array(all_samples_list[1]), np.array(all_samples_list[2]), np.array(all_samples_list[3]), np.array(all_samples_list[4]), np.array(all_samples_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data, cleaned_categories, cleaned_valid_periods = clean_data(training_data)\n",
    "\n",
    "cleaned_categories.shape, cleaned_data.shape, cleaned_valid_periods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A, data_B, data_C, data_D, data_E, data_F = split_for_categories(cleaned_data, cleaned_categories)\n",
    "\n",
    "data_A.shape, data_B.shape, data_C.shape, data_D.shape, data_E.shape, data_F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_ratio = 0.8):\n",
    "    data = data\n",
    "    \n",
    "    rows_to_select = int(len(data) * train_ratio)\n",
    "    training_set = data[:rows_to_select, :]\n",
    "    test_set = data[rows_to_select:, :]\n",
    "    \n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_D, test_D = train_test_split(data_D)\n",
    "train_D, validation_D = train_test_split(train_validation_D)\n",
    "len(train_D), len(validation_D), len(test_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, window=200, stride=1, telescope=9):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.copy()\n",
    "    padding_check = df.size%window\n",
    "\n",
    "    #print(temp_df.size)\n",
    "\n",
    "    if(padding_check != 0):\n",
    "        # Compute padding length\n",
    "        padding_len = window - df.size%window\n",
    "        padding = np.zeros((padding_len), dtype='float32')\n",
    "        temp_df = np.concatenate((padding,df))\n",
    "        assert temp_df.size % window == 0\n",
    "\n",
    "    #print(temp_df.size)\n",
    "    for idx in np.arange(0,temp_df.size-window-telescope,stride):\n",
    "        dataset.append(temp_df[idx:idx+window])\n",
    "        labels.append(temp_df[idx+window:idx+window+telescope])\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is a pandas series containing 48000 lists (either training, validation or test)\n",
    "def THE_SEQUENCER(data):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        time_series = data[i][valid_periods[i][0]:valid_periods[i][1]]\n",
    "        dset, labs = build_sequences(time_series)\n",
    "        if len(dset) == 0:\n",
    "            continue\n",
    "        dataset.append(dset)\n",
    "        labels.append(labs)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_D, train_labels_D = THE_SEQUENCER(train_D)\n",
    "validation_sequences_D, validation_labels_D = THE_SEQUENCER(train_D)\n",
    "test_sequences_D, test_labels_D = THE_SEQUENCER(data_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CONV_LSTM_model(input_shape, output_shape, num_neurons):\n",
    "    # Ensure the input time steps are at least as many as the output time steps\n",
    "    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n",
    "    # Define the input layer with the specified shape\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "    # Add a Bidirectional LSTM layer with 64 units\n",
    "\n",
    "    x = tfkl.Bidirectional(tfkl.LSTM(num_neurons, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
    "    \n",
    "    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n",
    "    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
    "    # Add a final Convolution layer to match the desired output shape\n",
    "    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n",
    "    # Calculate the size to crop from the output to match the output shape\n",
    "    crop_size = output_layer.shape[1] - output_shape[0]\n",
    "    # Crop the output to the desired length\n",
    "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
    "    # Construct the model by connecting input and output layers\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
    "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(train_sequences_D)):\n",
    "    for j in range(len(train_sequences_D[i])):\n",
    "        X_train.append(train_sequences_D[i][j])\n",
    "        y_train.append(train_labels_D[i][j])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = []\n",
    "y_valid = []\n",
    "for i in range(len(validation_sequences_D)):\n",
    "    for j in range(len(validation_sequences_D[i])):\n",
    "        X_valid.append(validation_sequences_D[i][j])\n",
    "        y_valid.append(validation_labels_D[i][j])\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "input_shape = (200,1)\n",
    "output_shape = (9,1)\n",
    "\n",
    "def objective_function(optuna_trial):\n",
    "\n",
    "    # Generate our trial model.\n",
    "\n",
    "    num_neurons = optuna_trial.suggest_int(\"num_neurons\", 10, 128)\n",
    "    batch_size = optuna_trial.suggest_int(\"batch_size\", 32, 128)\n",
    "\n",
    "    print(f\"Current trial parameters: num_neurons={num_neurons}, batch_size={batch_size}\")\n",
    "\n",
    "    model = build_CONV_LSTM_model(input_shape, output_shape, num_neurons)\n",
    "    \n",
    "    # Define callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', mode='min', patience=12, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    # Fit the model on the training data.\n",
    "    # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
    "    model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size, \n",
    "    epochs = 300,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    return model.evaluate(X_valid, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_study = optuna.create_study(direction=\"minimize\")\n",
    "optuna_study.optimize(objective_function,\n",
    "                    n_trials = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optuna_study.best_trial.params)\n",
    "print(optuna_study.best_trial.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
